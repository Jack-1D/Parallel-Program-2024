---
title: HW1

---

# HW1
> 112062706 林泳成

[ToC]

## Implementation
### 1. Handle arbitrary number of input items and processes
![截圖 2024-10-28 上午10.54.48](https://hackmd.io/_uploads/H1gRTung1l.png)
由於每個process都會需要知道自己和rank+1 process的資料量，所以需要計算自己的資料量dataSize(rank較低的process可能會比平均item數還多一個)，以及rank+1 process的資料量nxtSize(直接計算下一個process的資料量)

如果item數量比process少，那就只需要開item數量個process就好(用involveSize表示)
並且把會參與運算的process全部加到一個新的communicator裡，後面溝通就不會卡到沒參與的process

![截圖 2024-10-28 上午10.55.44](https://hackmd.io/_uploads/SyHDqbax1l.png)
beginOffset用來記錄讀檔的起始點
若dataSize == 0，process就不需要宣告儲存item的空間，也不需要讀檔

### 2. Sorting
1. Sort in each process
![截圖 2024-10-28 上午10.56.06](https://hackmd.io/_uploads/HJzRJKhekg.png)
為了要做merge sort，每個process內部的item必須先排序好，才能開始merge
原本是用qsort，後來聽很多人都用spreadsort，所以去簡單看了一下
[spreadsort](https://www.boost.org/doc/libs/1_86_0/libs/sort/doc/html/doxygen/boost_sort_c___reference/namespaceboost_1_1sort_1_1spreadsort_1a51394cf971edadc344af184931559053.html)
spreadsort好像是用bit運算硬體加速達到O(n)的時間複雜度
spreadsort也可以直接根據參數的type等價於integer_sort、string_sort、float_sort

2. Sort between process
![截圖 2024-10-28 上午11.02.58](https://hackmd.io/_uploads/S1zJlY2gJg.png)
發送item array的process和接收的process會做merge
小的item會在最後複寫在接收的process裡，大的會寫回給發送的process
如果發現小的item都和接收process長一樣就會紀錄**nonSwapped = 1**，作為後續的終止判斷
![截圖 2024-10-28 晚上8.45.50](https://hackmd.io/_uploads/BJu5_-plkx.png)
實作even phase和odd phase
odd phase的odd rank需要檢查第0個process不發送訊息

**優化merge過程**
![截圖 2024-10-29 下午4.40.13](https://hackmd.io/_uploads/SkW5k7Cgke.png)
![截圖 2024-10-29 下午4.40.27](https://hackmd.io/_uploads/SyW917Clkx.png)
不完全merge兩個array，而是指merge到dataSize長度就好
就可以省下一些array的讀寫
同時讓每個process都merge自己需要的array，增加資源使用效率


### 3. End detection
![截圖 2024-10-28 晚上8.55.57](https://hackmd.io/_uploads/Byflcbpx1g.png)
每次merge完都會做一次Allreduce檢查process之間是否有交換item，若都沒有交換就表示已經sort完成


### 4. Other implementation detail
![截圖 2024-10-28 晚上9.00.23](https://hackmd.io/_uploads/SyxOxoWaxkl.png)
有試著把Send和Recv合成一個Sendrecv，但好像沒有太大差別，還是需要等訊息
**優化後**
![截圖 2024-10-29 下午4.43.22](https://hackmd.io/_uploads/SJ6NeX0eye.png)
不確定效能實際上能好多少，至少可以優化後的方法可以讓每個process都呼叫Sendrecv

## Experiment & Analysis
### Performance matrix
![截圖 2024-10-29 下午3.54.15](https://hackmd.io/_uploads/SkWIbmCgkg.png)

時間計算方式：
Read file time: rank0讀檔案的時間
Computation time: rank0做sort和merge的總時間
Communication time: rank0做send、recv和allreduce的總時間
Write file time: rank0寫檔案的時間

簡單取幾個testcase裡的測資來做實驗
實驗一：
發現當每個process分到的item越多時，computation time會大幅提升，接著是communication time，然後是read file time。 Write file time沒有明顯提升。

實驗二：
相同數量的item分給相同數量的process，但開的node數量不一樣會導致computation time大幅下降，應該是因為計算loading分散給其他node來做(所以以35.txt和40.txt來看，node變成三倍，計算時間也減少三倍)。但代價是communication time些微上升，大概是因為跨node的通訊overhead較大導致。

**優化後**
![截圖 2024-10-29 下午3.53.41](https://hackmd.io/_uploads/S13zW7RgJx.png)


### Profile
![截圖 2024-10-29 下午4.51.09](https://hackmd.io/_uploads/rJfzzXCekg.png)
使用nsys做profiling
整體看起來最大的效能瓶頸在等待計算的blocking receive，因為較大rank的process在把array傳出去之後就會陷入blocking receive狀態，也不做其他事情，導致有將近一半的計算資源閒置。因次若能有個方法能讓這些process不要陷入blocking receive的狀態，而是負責處理一部分的運算就會讓整體的運算使用率提升，並提高speedup factor。

**優化後**
![截圖 2024-10-29 下午4.36.24](https://hackmd.io/_uploads/SJ06Z7Cx1g.png)
因為寫memory的次數變少，所以等待時間有明顯變少。

![截圖 2024-10-29 下午5.04.13](https://hackmd.io/_uploads/H1Vmr7Rekl.png)
用event view簡單看了一下發現撇除等待時間不看，花最多時間的是Send、Recv和兩者之間的Barrier。這代表如果能減少溝通輪數，不只可以減少寫memory的次數，還能因為減少這些API call來小幅減少呼叫時間。

### Speedup
![截圖 2024-10-29 凌晨12.04.07](https://hackmd.io/_uploads/BkmfUV6lke.png)
Speedup factor實驗使用testcase的35.txt當作input(因為執行時間最久，但又不會太久)
實驗條件從1個process到64個process
左圖計算不同process下的I/O time、Computation time、Communication time
右圖計算不同process下的speedup factor，並和ideal speedup factor比較

**優化後**
![截圖 2024-10-29 下午4.35.51](https://hackmd.io/_uploads/r1Q8fQAl1l.png)
可以明顯看出計算時間大幅度下降，通訊時間逐漸變為bottleneck
增加1~8個process的實驗用來檢查node數量相同的情況下，計算時間有逐漸下降，但通訊時間會忽高忽低
雖然speedup factor有比起優化前往上了一點，但是離ideal speedup factor還差非常多

### Discussion
1. 當input逐漸scale up，computation cost會大幅上升。主要的bottleneck是對array的讀寫所造成的等待。因為大部分的merge過程都會需要對整個array做複寫，大量的複寫就會導致大量的等待時間。優化的可能考慮方式就是用vectorization，同時對多個item做操作。或是使用指標進行array element交換，避免array複寫。
2. 實驗結果指出scalability很差，因為計算輪數太多，而每一輪都會使用到memory複寫和process之間的溝通，因此computation time和communication time都降不下去。若能有個方法能讓這些process不要陷入blocking receive的狀態，而是負責處理一部分的運算就會讓整體的運算使用率提升，並提高speedup factor。我認為閒置資源越少，就越能逼近ideal speedup factor。
優化後有些為提升運算資源的使用率、些微減少等待時間，但是這種實作方式會讓odd phase和even phase輪數固定。也就是必須要降低每一輪的memory讀寫時間才能夠盡量逼近ideal speedup factor。

## Conclusion
隨著課堂的進行，我發現這隻程式其實有很多可以優化的地方。例如減少if else的數量，也就是減少branch的產生，不然不需要做該branch的process就必須要等待要做的process做完才能接著做下去。又或是減少複寫memory的次數也能大幅減少memory store產生的等待overhead。因此不論是使用指標交換array元素或是使用特殊register來減少複寫memory都能大幅加速程式的執行。
從這次的作業我發現即使知道效能瓶頸發生在哪裡，要如何有效去優化這些部分又是一個很大的學問。
實作中遇到的最大問題是一開始不知道是要用merge sort來實作，結果導致我寫了一個時間複雜度超高的交換演算法。所以花了很多時間重新理解題目並重新用merge sort來實作才免於超時。

