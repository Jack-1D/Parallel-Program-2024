---
title: HW2

---

# HW2
> 112062706 林泳成

[ToC]

## Implementation
### pthread
![截圖 2024-11-01 上午11.16.18](https://hackmd.io/_uploads/r1i7uabWke.png)
![截圖 2024-11-01 上午11.16.35](https://hackmd.io/_uploads/H17qop--kl.png)
每個thread都會在while迴圈內去拿新的工作，工作大小定義在global variable(blockPerThread)。每個thread都會一直拿工作直到沒有新的工作可以拿。jobRemain function會回傳可以拿的資料量。另外有用curX和curY指向下一個工作的起始位置，所以會用lock把這個curX和curY的更新包起來。
![截圖 2024-11-01 中午12.04.47](https://hackmd.io/_uploads/HJsoXAb-1l.png)
如果拿到的工作是最後一份，那就讓他直接執行
剩下的部分會用vectorization實作
![截圖 2024-11-01 下午2.46.51](https://hackmd.io/_uploads/rkJGqgGZke.png)
先把前8個資料塞進__m512d的vector，之後就同時更新8個變數
![截圖 2024-11-01 下午1.55.53](https://hackmd.io/_uploads/B1ik9ezZ1l.png)
![截圖 2024-11-01 下午2.47.55](https://hackmd.io/_uploads/SkYeqlGWJe.png)
每次都去檢查看看有沒有任務跑完了，如果有就把那一格的repeats次數寫出來，並且把新的工作覆蓋在空出來的格子上
![截圖 2024-11-01 中午12.14.07](https://hackmd.io/_uploads/SyLiHCWbkx.png)
直到剩下最後7個任務無法繼續用vectorization，就重新sequential得跑

#### 優化後
![截圖 2024-11-01 下午2.48.09](https://hackmd.io/_uploads/HyKdslGb1x.png)
其實後來發現vectorization不一定要全滿才能做，裡面也可以塞無用的空直，這樣就不需要重跑最後7個任務了(這個優化因為是每次拿新工作就會在最後執行到，所以跑judge的時候總共快了16秒左右)
![截圖 2024-11-01 下午3.07.32](https://hackmd.io/_uploads/SJJtCxGWkg.png)
![截圖 2024-11-01 下午3.07.46](https://hackmd.io/_uploads/rkJYAgfbke.png)
還有一個小優化是在最後拿不滿blockPerThread的工作量時才會做的sequencial run。如果也把這些塞不滿8格的工作也拿去做vectorization就可以不需要一個一個慢慢做(這個優化只優化最後一個工作，所以進步幅度有限，最後跑judge總共也只快了0.3秒)
![截圖 2024-11-01 下午3.24.56](https://hackmd.io/_uploads/SyeQOGWMWkg.png)
後來發現如果圖片足夠大，那最後7個點的repeats次數一定會比重新計算(curX=0, curY+=1)還要早算完，因此就不需要另外把不合法的工作排除，也就不需要開mask來遮掉有問題的工作(跑judge快了2秒)

### hybrid
![截圖 2024-11-02 晚上8.15.49](https://hackmd.io/_uploads/rJlyKqXZyg.png)
每個process都會分到rows行的工作。如果無法整除，前面幾個process會拿到多一行的工作
![截圖 2024-11-02 晚上8.16.24](https://hackmd.io/_uploads/B1Q3tqQZ1g.png)
OpenMP開出來的thread執行的工作基本和pthread一樣，都用vectorization去優化計算
每個process實際上會是輪流拿每一行的工作
![截圖 2024-11-02 晚上8.16.51](https://hackmd.io/_uploads/S1hXq9Q-1g.png)
![截圖 2024-11-02 晚上8.17.03](https://hackmd.io/_uploads/Bk-H5cXb1g.png)
剩餘無法塞進__m512d的工作就在最後用sequencial去做
![截圖 2024-11-02 晚上8.26.29](https://hackmd.io/_uploads/HJKTi5X-Jg.png)
![截圖 2024-11-02 晚上8.26.43](https://hackmd.io/_uploads/rJFpscmbJl.png)
由於每個process是跳著拿工作的，且工作量都不一樣，所以直接用MPI_Gather不是個好方法
因此使用MPI_Gatherv來整合整張圖
MPI_Gatherv會需要多吃兩個參數，一個是用來存每個rank要傳的data量，一個是buffer接收每個rank的資料時的起始位置
![截圖 2024-11-02 晚上8.36.40](https://hackmd.io/_uploads/ByK165m-1e.png)
Gather完的圖片會是每個process跳著拿工作的結果
所以需要再跳著把圖片組回來

#### 優化後
![截圖 2024-11-03 下午4.11.03](https://hackmd.io/_uploads/r1f8ehEWJx.png)
因為OpenMP的data parallelism需要把程式改成互相independent的for迴圈，所以我把原本的程式改成能用for迴圈來增加startY，讓原本會有dependecy的程式能分離，讓OpenMP自動做工作分配(我原本跑judge要303秒，這個優化讓這隻程式變成跑judge只需要127秒。**突然發現OpenMP很吃programmer的使用**)


## Experiment & Analysis
### Performance matrix & Speedup
#### pthread
因為看到strict28到strict36都是一樣的iteration、圖寬和圖高，就只差在x0, x1, y0, y1，就想說拿來當作實驗input。結果發現時間差很多，最快的6秒就完成了，最慢的要107秒。
![截圖 2024-11-02 晚上11.13.45](https://hackmd.io/_uploads/HJ9s2pmWye.png)
實驗簡單測了各個input在不同數量thread和process的情況下能加速多少
實驗結果也明確顯示multi-process在這種實作下是無法加速的(因為只有使用pthread)
上圖中multi-process的各個執行時間是取所有process中最慢的
其中**Initialization time**用來計算初始化區域變數的時間
**Thread computation time**用來計算從fork所有thread到join完所有thread所花時間
**Write PNG time**用來計算最後畫圖的時間
![截圖 2024-11-02 晚上11.22.44](https://hackmd.io/_uploads/ByOeTaQZkg.png)
![截圖 2024-11-02 晚上11.24.29](https://hackmd.io/_uploads/B1OgaaQZkx.png)
![截圖 2024-11-02 晚上11.25.26](https://hackmd.io/_uploads/By_eT6QZkl.png)
![截圖 2024-11-02 晚上11.26.59](https://hackmd.io/_uploads/Bk_gT6mZkx.png)
![截圖 2024-11-02 晚上11.27.56](https://hackmd.io/_uploads/H1uxa6XZkx.png)
![截圖 2024-11-02 晚上11.29.06](https://hackmd.io/_uploads/S1lugTaQZye.png)
![截圖 2024-11-02 晚上11.30.08](https://hackmd.io/_uploads/H1g_xTaXZ1l.png)
![截圖 2024-11-02 晚上11.31.07](https://hackmd.io/_uploads/HkOgT6mbJe.png)
![截圖 2024-11-02 晚上11.31.53](https://hackmd.io/_uploads/BJdl6TXbke.png)
以上幾張圖顯示初始化時間根本可以忽略。當每個thread的computation time很大時，write PNG time的佔比會非常小。
因此可以發現當總執行時間非常長時，我實作的平行化計算能夠逼近ideal speedup factor
也就是說如果撇除掉initialization time和write PNG time這兩個我沒做平行化的部分，其餘時間的平行化程度很高(可以從strict34.txt看出來)
#### hybrid
經過pthread的實驗後，發現只要測strict34.txt就可以展現這隻程式的平行度優化結果(因為可平行化的部分佔比很大)
**Initialization time**用來計算初始化區域變數的時間
**OpenMP time**用來計算#pragma內的程式執行時間，由於這段會同時受process和thread數量影響，所以直接把這段當成是可平行化的最大部分
**Communication time**用來計算MPI_Gatherv所花時間
**Gather image time**用來計算把MPI_Gatherv組起來的圖片重組花的時間
**Write PNG time**用來計算最後畫圖的時間
![截圖 2024-11-03 下午5.35.31](https://hackmd.io/_uploads/HkMeHT4-yx.png)
上圖是固定1個thread，實驗process從1~12個
左圖是上述各種耗時的佔比圖
中間是總時間的speedup factor和ideal speedup factor的比較
右圖是如果不算無法平行的部分，也就是只算#pragma內的程式的speedup是否可以逼近ideal speedup factor
![截圖 2024-11-03 下午5.35.43](https://hackmd.io/_uploads/H1zxB6Eb1x.png)
上圖是固定1個process，實驗thread從1~12個
![截圖 2024-11-03 下午5.35.55](https://hackmd.io/_uploads/BJbxraVb1l.png)
上圖是同時增加process和thread的數量看看speedup factor是否能呈指數成長
實驗結果大概在5個process和5個thread時就上不去了

### Profile
#### pthread
![截圖 2024-11-03 下午3.00.05](https://hackmd.io/_uploads/HyK5kjVZJl.png)
從profiler可以看出幾個點：
1. 有些thread中間會有一小段空白(約8ms)，這些應該是做完工作準備拿新工作時，別的thread已經搶到lock，所以是等待lock的時間。
2. 有時候thread在等lock，原本在使用的CPU core會被其他thread搶佔，所以等拿到lock後，會交換執行的CPU core。
![截圖 2024-11-03 下午3.11.23](https://hackmd.io/_uploads/SJWwzjNWkg.png)
![截圖 2024-11-03 下午3.12.12](https://hackmd.io/_uploads/B1GwzoVbke.png)
每個thread做完一次工作大概是0.0004352秒

#### hybrid
![截圖 2024-11-03 晚上7.49.04](https://hackmd.io/_uploads/S1CwLyS-ye.png)
Hybrid方法就不像pthread會用lock去管理工作，而是在MPI_Init後就針對不同rank的process分配好需要做的工作，因此不會有中間的小斷點

### Discussion
**Scalability**
拿strict34.txt當作input來計算從1個thread scale up到12個thread。
![截圖 2024-11-03 凌晨12.57.55](https://hackmd.io/_uploads/r1Wc507b1g.png)
實驗結果發現當thread數量逐漸scale up，speedup factor逐漸趨緩，看起來會收斂到一個最終值
但是我又另外做了一個實驗，只計算可平行化的部分speedup會不會也收斂
結果發現當process在12個以下時，沒辦法看出明顯得收斂，甚至接近ideal speedup factor
因此可以得到一個結論，撇除可平行化的部分，剩餘不可平行化的部分就是無法逼近ideal speedup factor的bottleneck

**Load balancing**
拿strict34.txt當作input來實驗8個thread的執行時間
![截圖 2024-11-03 上午10.44.43](https://hackmd.io/_uploads/SknJVDE-1x.png)
**Pthread**實驗結果顯示，每個thread執行時間很平均，算是達成一個不錯的load balance
藉由控制blockPerThread來達到更fine-grained的工作分配是可行的
然而我做了許多小實驗，從blockPerThread = 30，做到blockPerThread = 2500，跑judge的總時間是漸減的。由此可知，當工作被切得越小份時，雖然能增加計算資源的利用，但是每次重新分配資源到vector所產生的overhead也會相對提升。因此當圖片越大時，blockPerThread就要相對上升，以減少overhead的產生
![截圖 2024-11-03 晚上7.59.48](https://hackmd.io/_uploads/H1x0S1rZkg.png)
**Hybrid**方法因為rank0要負責做圖片的aggregate，所以會花比較多時間，因此會成為效能瓶頸
其餘的process執行時間都差不多，算是達成不錯的load balance
以目前簡單做的小實驗來看，data parallelism的schedule用static、branch = 10效果最好

## Conclusion
經過這次作業，我了解到不是每個程式都適合拿來做平行計算優化。這次作業的程式就很適合用平行計算優化。從sequential的程式來看，每個計算工作都互相獨立，且性質相同，非常適合同時做計算。不論是每個工作的計算能平行，還是最後畫圖的部分，我相信都是可以用平行計算去逼近ideal speedup的。
實作上遇到比較大的困難大概是看不到vector裡的值，所以debug的時候會需要把vector裡的所有元素都倒出來，會需要看一堆log來檢查每個工作是不是都做了，也需要檢查每個工作是不是在正確的時間結束工作。