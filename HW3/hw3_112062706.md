---
title: HW3

---

# HW3
> 112062706 林泳成

[ToC]

## Implementation
### HW3-1
![截圖 2024-11-13 晚上11.02.01](https://hackmd.io/_uploads/rytl5yQz1l.png)
原本是用openMP直接對blocked floyd warshall做修改。但是好像是每個子工作太大，沒辦法做到太fine grained
![截圖 2024-11-14 中午12.56.07](https://hackmd.io/_uploads/B1izVWQzJx.png)
用nsys看執行狀況發現等待的情況很嚴重，應該是子工作太大導致的
這個方法因為SIMD包住的範圍包含了三個額外的for迴圈做陣列更新，即便是用schedule(dynamic)，也會因為一個thread跑一次子工作太久導致所有thread都在等待
每次結束SIMD的迴圈時，都會額外有一個process獨自執行，其他process都沒工作。好像是因為openMP在做完SIMD操作後會附上一個barrier，之後會由主程式做資源管理，負責在runtime做資源回收和調度
所以後面優化就是想優化thread資源分配的overhead
如果只在程式一開始初始化一次thread就可以避免中間執行花太多時間在資源管理

#### 優化後
![截圖 2024-11-13 晚上10.31.17](https://hackmd.io/_uploads/Sk8jW-mGJe.png)
![截圖 2024-11-13 晚上10.31.26](https://hackmd.io/_uploads/S1vjZb7fkg.png)
由於之前的做法工作沒辦法切得很好，會有空隙，所以想說用pthread在每一輪就先把工作切得剛剛好，每個thread在每一輪就是處理ceil(n*n, NUM_THREAD)的工作量
結果表現好像沒有太好，其中一個部分是每個thread被fork出來的時間是由for loop一個一個分配出來的，所以先執行的thread就需要等待較慢初始化的thread
![截圖 2024-11-14 下午4.18.56](https://hackmd.io/_uploads/SJVqM4XMkl.png)
每一輪計算都會花5毫秒

![截圖 2024-11-13 晚上10.30.47](https://hackmd.io/_uploads/S1yHMZXMke.png)
因為之前的做法每一輪的計算會花很多時間，所以後來去看了cache要怎麼存取才會快
就發現主要是cache miss花太多時間，就連之後又測試了__builtin_prefetch都沒辦法在不同thread之間maintain cache，導致整體access時間增加
所以就改成每個thread同時只能抓一行的陣列來做運算，結果大幅改善cache miss的問題
![截圖 2024-11-14 下午4.18.45](https://hackmd.io/_uploads/HkIJXE7Mke.png)
每一輪計算只花4.7毫秒

### HW3-2、HW3-3
![截圖 2024-12-10 晚上8.04.49](https://hackmd.io/_uploads/Sk220oBVJx.png)
![截圖 2024-12-10 晚上8.06.00](https://hackmd.io/_uploads/rkhnRiSNyl.png)
資料的分配上都是照著每個phase要計算的工作去切
只有在HW3-3是把整個母工作分成上下兩大塊個別計算
HW3-2和HW3-3我都是選擇blocking factor 64、block裡的thread都是(32X32)。
Blocking factor選64是因為一個block是32X32個thread，而且share memory又可以長寬各開到64X64，所以在嘗試32和64兩種blocking factor後選擇效能比較好的64。
至於block的thread分配則是選擇講義裡建議的x, y各是32倍數。
至於grid的分布則是把graph的長寬除以block大小。
![截圖 2024-12-05 上午11.16.17](https://hackmd.io/_uploads/BkxRW3B4Jl.png)

![截圖 2024-12-10 晚上8.21.26](https://hackmd.io/_uploads/HkhUzhr4Jg.png)
HW3-3因為會把整個graph切成上下兩大塊，所以在溝通上會是根據每一輪，擁有那一round資料的人把資料丟出來給沒有的GPU拿。這樣就可以讓兩個GPU平行計算phase 3，但是phase 1因為只需要一個GPU算，phase 2因為工作量少所以效果都有限。最後再把兩個GPU上的graph組合起來就好了。

實作上有試著改變access pattern來解決bank conflict(因為phase 2會剛好用完share memory，就不能用padding)，但是效果很差。
其餘部分就是分工把整塊block搬到share memory上。
![截圖 2024-12-10 晚上8.29.59](https://hackmd.io/_uploads/ry3pr3HVkg.png)
也有使用stream把傳資料和計算分成兩個stream，但是效果也有限。因為總共只需要傳來回一次，就要為了這來回一次創建一個stream，導致變更慢。
![截圖 2024-12-10 晚上8.37.24](https://hackmd.io/_uploads/r1SnIhS4ke.png)
但是對於HW3-3多GPU來說因為每一輪都會溝通一次來回，所以多創一個stream就有價值，因此有顯著加速。

## Profiling Results
使用c10.1測資當作input，實驗不同blocking factor對GPU效能的各項指標有什麼影響
![截圖 2024-12-10 晚上7.20.36](https://hackmd.io/_uploads/BkrN4jSV1e.png)
**▲不同blocking factor影響GPU share memory在不同kernel call load和store操作的bandwidth**
![截圖 2024-12-10 晚上7.20.48](https://hackmd.io/_uploads/H1HV4sBVJl.png)
**▲不同blocking factor影響GPU warp的使用效率**
![截圖 2024-12-10 晚上7.21.01](https://hackmd.io/_uploads/HJL4NjHEkx.png)
**▲不同blocking factor影響GPU core的佔用率和Streaming Multiprocessor的使用效率**

## Experiment & Analysis

![截圖 2024-12-10 晚上7.20.04](https://hackmd.io/_uploads/HkSVEiBE1l.png)
**▲不同blocking factor影響GPU global memory load和store操作的bandwidth**
![截圖 2024-12-10 晚上7.20.19](https://hackmd.io/_uploads/B1BEVjSN1e.png)
**▲不同blocking factor影響GPU 每秒整數計算量**
### Optimization
用p11k1測資當作input資料，測試每多增加一種優化方法，能加速多少
![截圖 2024-12-10 晚上7.33.42](https://hackmd.io/_uploads/S1K5DirVye.png)
由於CPU會超時，所以實驗時是把(一輪的時間 X round)當作結果
CPU baseline是完全沒有優化的方法
GPU baseline是把每個pixel都丟給一個GPU thread計算，所以可以快很多倍
後面實作unroll、stream、bank conflict方法好像對整體影響不大，甚至有可能讓整體效能變差

### Time Distribution
根據不同的scale選擇了不同的測資當作input
#### 單GPU實驗
![截圖 2024-12-10 晚上7.40.18](https://hackmd.io/_uploads/HkSgFiB4Jl.png)
在計算工作複雜度較小時，I/O幾乎等於整體運算時間。但隨著vertex和edge的數量上升，計算量變大，計算才逐漸變成程式的bottleneck。

#### 單GPU跑AMD實驗
![截圖 2024-12-10 晚上7.40.29](https://hackmd.io/_uploads/BkjdFjSNkx.png)
一樣的程式轉成使用AMD的API，並跑在AMD的GPU上，運算速度竟然快了好多。當測到p40k1這筆測資時，I/O時間居然和計算時間沒差多少。而且在做GPU memory copy的時候，還發現nvidia的copy from host to device和device to host的時間差距其實蠻大的。AMD的API對這兩個操作所花的時間幾乎是一樣的。

#### 雙GPU實驗
![截圖 2024-12-10 晚上7.40.42](https://hackmd.io/_uploads/S1euKsHEkl.png)
多GPU和單GPU的狀況一樣，都是計算工作的scale決定計算時間和I/O的佔比。

#### 雙GPU跑AMD實驗
![截圖 2024-12-10 下午4.04.32](https://hackmd.io/_uploads/H1BRKsH4kx.png)
雙GPU跑編譯好的AMD code不知道為什麼會遇到不能用OpenMP的問題，所以就沒有測了。

#### Weak Scalability
![截圖 2024-12-10 晚上7.40.51](https://hackmd.io/_uploads/B1IgTor4yx.png)
對於單GPU和雙GPU而言差距不算太大，但因為實作的關係，多GPU在運作上還比單GPU慢一點點。
如果只看計算不看I/O，結果也是一樣。


## Conclusion
這次作業有讓我嘗試了很多上課教到的GPU優化技巧。經過不同的實作，我也發現不同的情境下並不是每個優化技巧都是用，有些技巧如stream會需要花時間創建一個stream，若stream只用沒幾次就不用了，會很浪費。又或是2D memory copy API會需要先pin host memory再對齊，才能讓API加速。如果要copy的資料很小且很雜，那麼pin memory加上對齊的時間就會搞垮整個效率，因此要慎重定使用的優化技巧。多GPU在實作後才發現，以目前所學不太有優化的空間，基本上沒有優化還可能比單GPU還糟。