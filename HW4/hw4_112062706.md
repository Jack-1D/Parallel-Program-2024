---
title: HW4

---

# HW4
> 112062706 林泳成

[ToC]

## Implementation
![截圖 2024-12-17 上午11.00.21](https://hackmd.io/_uploads/BkT8Yv0V1e.png)
由於batch可能會很大，SRAM的大小有限，所以flash attention會分batch操作。
![截圖 2024-12-17 上午11.05.34](https://hackmd.io/_uploads/B1_qcDANyl.png)
Inner loop是針對每個query q切一塊tile丟進SRAM做計算
Outer loop是把Key k和value v個別切一塊tile丟進SRAM做計算
每個batch也有自己的l和m要維護
進到kernel前會把k和v切成切成$bc * d$大小的tile，也會把q切成$N * d$大小的tile
![截圖 2024-12-17 上午10.53.13](https://hackmd.io/_uploads/SyxbjdCV1g.png)
kernel內部會同時把所有q都去對k和v做運算(q切成的tile會在kernel裡被放進share memory中，因此一個block就會擁有一個q的tile)
![截圖 2024-12-17 上午10.53.49](https://hackmd.io/_uploads/rk-jiuRVkl.png)
![截圖 2024-12-17 上午10.54.01](https://hackmd.io/_uploads/rJbiiuC4ke.png)
kernel內部就是把算有該做的操做都列出來，因為有些中繼資料(e.g. sij, mij, pij, lij)會有dependency，所以需要在做完這些操作後同步化block裡的threads
l和m會在最後一個步驟做更新。l和m會在每個qkv算完後更新，且每個batch會各自維護l和m

br和bc選擇和sequential版本一樣的32, 32，因為block的上限也是$32 * 32$個threads，而且這兩個維度最好都要是32的倍數

至於share memory和grid dimension的配置是根據flash attention的inner loop會需要計算整個$N * d$，而block size又是$32 * 32$也就是$tile\_size * tile\_size$，所以grid size要有一個維度是$\frac{N}{tile\_size}$
Share memory是根據flash attention的tile大小和input d的最大值來設定的
而這樣的設定會在d = 64時可以完整用到42240 bytes的share memory(硬體可支援上限： 49152 bytes)


## Profiling Results
由於profiling會增加額外的執行時間，所以如果跑的資料量太大會導致timeout，因此這裡實驗使用複雜度較低的測資。
由實驗可以看出N和d的維度會大幅影響整體計算時間，因為flash attention把大矩陣切成很多小矩陣，會產生很多矩陣的運算，所以當每個小矩陣越大計算時間就會增加很多。
![截圖 2024-12-17 晚上8.18.47](https://hackmd.io/_uploads/SJE9zekS1x.png)
**▲當資料的複雜度逐漸變大時，GPU global memory的使用量會逐漸上升，造成頻寬使用效率提升**
![截圖 2024-12-17 晚上8.19.03](https://hackmd.io/_uploads/B1BczxJr1e.png)
**▲當資料的複雜度逐漸變大時，GPU 每秒的計算量也會逐漸提升，也就是能提升硬體的使用效率**
![截圖 2024-12-17 晚上8.19.14](https://hackmd.io/_uploads/ryHqMlkBJe.png)
**▲當資料的複雜度逐漸變大時，能塞進share memory的資料變多，GPU share memory的使用量會逐漸上升**
![截圖 2024-12-17 晚上8.19.53](https://hackmd.io/_uploads/SJEqflJHyx.png)
**▲GPU warp內的平均使用效率不會隨著資料的複雜度變大而增加。但是由於kernel function內部會有一些工作是只有一部份的threads會做，所以免不了產生branch，所以warp efficiency沒有很高(記憶體優化已經盡量做到比較好了)**
![截圖 2024-12-17 晚上8.20.04](https://hackmd.io/_uploads/S1ScGl1BJl.png)
**▲GPU的佔用率和Streaming Multiprocessor的使用效率有隨著計算量上升而有顯著提升，代表GPU的資源有逐漸被佔滿**

## Experiment & Analysis
### Optimization
優化的實驗是使用t28的測資(B=4, N=16384, d=64)，因為這個複雜度的sequential運行時間很常，但是GPU優化可以跑得很快。
![截圖 2024-12-17 晚上8.20.21](https://hackmd.io/_uploads/HJVg_lyBye.png)
#### Share memory & Bank conflict
![截圖 2024-12-17 晚上9.24.30](https://hackmd.io/_uploads/SkypieJBJx.png)
把中繼的tile都放到share memory可以大幅減少access time，share memory也是flash attention最主要能表現好的原因。

為了解決share memory的bank conflict，在矩陣的最尾端加上一個padding就能簡單解決同一個thread一直access同一個bank的資料。

#### 2D API
![截圖 2024-12-17 晚上9.39.57](https://hackmd.io/_uploads/rkJrJWyHyl.png)
2D API會先把記憶體補滿，這樣整個二維陣列就會是連續記憶體，複製起來也會比較快。

#### unroll
![截圖 2024-12-17 晚上9.41.40](https://hackmd.io/_uploads/Bych1-yB1l.png)
有for迴圈就可以unroll一夏，只是因為不知道輸入的batch數量，所以就塞了一個中間值。基本上沒差多少。

#### Coalesced memory
![截圖 2024-12-17 晚上9.44.42](https://hackmd.io/_uploads/By2Ig-kBJl.png)
因為brIdx比較常在陣列第一個維度的索引值，所以就讓相同y，不同x的thread去access會比較快。

#### Register
![截圖 2024-12-17 晚上9.48.53](https://hackmd.io/_uploads/BJCP--kB1g.png)
![截圖 2024-12-17 晚上9.49.06](https://hackmd.io/_uploads/BJ0PWWkByg.png)
![截圖 2024-12-17 晚上9.49.15](https://hackmd.io/_uploads/HkCwb-yBJx.png)
因為register的access time會比share memory的array快一點點，所以迴圈中會重複使用到的share memory access就改成register。

#### 1 Stream & 4 Stream
![截圖 2024-12-17 晚上9.51.56](https://hackmd.io/_uploads/rkYzM-kr1g.png)
![截圖 2024-12-17 晚上9.52.07](https://hackmd.io/_uploads/r1tfMZ1Syx.png)
由於在程式一開始會需要把host的Q, K, V, O搬到device，所以如果都在stream 0搬會沒辦法產生overlap。因此接下來要決定的就是要針對每一個copy function都創建一個stream還是只用一個額外stream就好，畢竟只複製一次(等於是create stream的overhead和實際搬移資料所花費時間之間的選擇)。實驗結果是大部分測資都會需要搬一小段時間，而這個overlap的時間有超過create stream所花費時間。所以選擇用4個stream針對每個array都創建一個stream來搬資料。

#### Other graph (Execution time speedup)
![截圖 2024-12-17 晚上9.59.17](https://hackmd.io/_uploads/rkFpm-yHJl.png)
使用flash attention把array切成多個tile並使用GPU來平行access搬到share memory裡的資料可以加速原本的sequential code很多倍。
當資料的複雜度上升(尤其是N和d上升)時，CPU sequential的執行時間上升幅度很大。但當使用GPU加速時，執行時間上升幅度不會這麼大，而且也不一定會隨著N和d上升執行時間就跟著上升。以t15測資為例，sequential會因為整體資料的複雜度上升而上升，但是在GPU的情境下t15執行時間卻比t20慢。主要原因應該也是因為N和d的上升對sequential很致命，但對GPU來說不會，因此B上升太多才導致t15算得比較久。

## Conclusion
從把sequential flash attention改成GPU code的過程中，我學到了tile algorithm的實際操作方式，也瞭解了在頻繁access 較小array的情況下share memory會比global memory快上很多。因此當問題的array較大時，把大array切成小tile能加速access速度很多。
從這次作業中也發現coalesced memory比起其他加速手法真的能快上很多倍。另一個加速的重點是在可容許些微錯誤的情境下，使用合理的flag來優化浮點數計算速度，也可以加速計算很多。